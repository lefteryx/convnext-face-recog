{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install timm\n",
    "# %pip install onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO\n",
    "- [X] Train/Test Split\n",
    "- [X] Normalization\n",
    "- [X] Data Augmentation\n",
    "- [ ] Hyperparameter Tuning\n",
    "- [ ] Figure out ONNX Verification, Inference\n",
    "- [ ] Export to TensorFlow.js?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOutliers?\\nhttps://kevinmusgrave.github.io/pytorch-metric-learning/\\nhttps://github.com/KevinMusgrave/pytorch-metric-learning/blob/master/examples/README.md\\nhttps://colab.research.google.com/github/KevinMusgrave/pytorch-metric-learning/blob/master/examples/notebooks/SubCenterArcFaceMNIST.ipynb#scrollTo=GJ_L0TrTDnEA\\n---> Get_Outliers()\\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Outliers?\n",
    "https://kevinmusgrave.github.io/pytorch-metric-learning/\n",
    "https://github.com/KevinMusgrave/pytorch-metric-learning/blob/master/examples/README.md\n",
    "https://colab.research.google.com/github/KevinMusgrave/pytorch-metric-learning/blob/master/examples/notebooks/SubCenterArcFaceMNIST.ipynb#scrollTo=GJ_L0TrTDnEA\n",
    "---> Get_Outliers()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training ConvNext-Atto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from pytorch_metric_learning import losses, testers\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "\n",
    "import timm\n",
    "\n",
    "import onnx\n",
    "import onnxruntime\n",
    "\n",
    "from PIL import UnidentifiedImageError\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "if log:\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 500\n",
    "learning_rate = 1e-3\n",
    "loss_lr = 1e-4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "num_classes = 20 # ~100*12\n",
    "embedding_size = 320\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "if log:\n",
    "    writer.add_scalar('Hyperparameters/Batch_size', batch_size, 0)\n",
    "    writer.add_scalar('Hyperparameters/Epochs', epochs, 0)\n",
    "    writer.add_scalar('Hyperparameters/Learning_rate', learning_rate, 0)\n",
    "    writer.add_scalar('Hyperparameters/Loss_lr', loss_lr, 0)\n",
    "    writer.add_scalar('Hyperparameters/Num_classes', num_classes, 0)\n",
    "    writer.add_scalar('Hyperparameters/Embedding_size', embedding_size, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shouldn't really throw an error, but just in case\n",
    "class RobustImageFolder(datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        try:\n",
    "            sample = self.loader(path)\n",
    "        except UnidentifiedImageError:\n",
    "            print(f\"\\033[91mSkipping Corrupt Image:\\033[0m {Path(path)}\")            \n",
    "            # return None, None\n",
    "            return self.__getitem__(index + 1)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return sample, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RobustImageFolder('../faces/split/test', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = RobustImageFolder('../faces/split/val', transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXtArcFace(nn.Module):\n",
    "    def __init__(self, model_name, embedding_size, pretrained=False):\n",
    "        super(ConvNeXtArcFace, self).__init__()\n",
    "        self.convnext = timm.create_model(model_name, pretrained=pretrained)\n",
    "        self.convnext.reset_classifier(num_classes=0, global_pool='avg')\n",
    "      \n",
    "    def forward(self, x):\n",
    "        x = self.convnext.forward_features(x) # \n",
    "        x = F.avg_pool2d(x, 7).flatten(1)\n",
    "        print(\"Embeddings:\", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'convnextv2_atto'\n",
    "model = ConvNeXtArcFace(model_name, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: torch.Size([1, 320])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-6.8194e-02,  5.1333e-02, -6.1956e-02, -3.6360e-02,  5.3770e-02,\n",
       "         -8.9238e-02, -1.2089e-01, -7.4126e-02, -1.1603e-02, -5.8732e-02,\n",
       "         -1.4981e-02,  3.6882e-02, -1.0532e-02,  8.5154e-02,  1.1332e-01,\n",
       "         -8.5254e-02,  2.5808e-03, -1.2512e-01,  6.8433e-02, -1.1143e-01,\n",
       "         -5.7732e-02,  6.4791e-02, -2.9316e-03,  5.4968e-02, -7.3018e-03,\n",
       "          1.0388e-01,  1.8839e-01,  5.7166e-02, -2.2003e-01, -4.3469e-02,\n",
       "          1.7603e-02, -5.1367e-02,  5.9913e-02, -4.6071e-03, -1.9538e-02,\n",
       "         -1.6529e-05,  2.3502e-02, -4.2935e-02,  6.5636e-02, -2.4650e-01,\n",
       "         -9.0053e-02, -1.6097e-01, -1.6759e-03, -6.9118e-03,  5.4979e-02,\n",
       "         -7.1678e-03, -6.4141e-02,  5.7802e-02, -7.1169e-02, -2.8143e-01,\n",
       "          2.3036e-01,  4.9556e-02, -8.6411e-02,  1.8848e-03, -8.5480e-02,\n",
       "         -1.0621e-01, -1.9180e-01,  2.6773e-02, -1.0937e-01, -5.4265e-02,\n",
       "         -8.3141e-02,  2.5344e-01, -2.3842e-02, -7.4361e-02,  7.1475e-02,\n",
       "          6.3373e-03,  7.4410e-02, -6.9111e-02,  1.0748e-01, -1.7510e-01,\n",
       "         -7.1547e-02, -4.5163e-02, -4.7472e-02,  4.5880e-02, -5.2759e-03,\n",
       "          5.1125e-02,  3.1934e-02,  8.9149e-02,  1.5799e-01, -1.8419e-01,\n",
       "          2.1024e-01, -1.4355e-02, -9.7016e-02, -5.4867e-02,  7.0090e-02,\n",
       "         -7.7331e-03,  5.0248e-02, -2.6676e-02,  2.1595e-02,  1.1299e-01,\n",
       "          1.4933e-01, -1.1324e-01,  1.8875e-01, -3.4387e-02, -7.5967e-02,\n",
       "         -5.4649e-02,  1.2440e-01, -1.2207e-01,  1.0355e-01, -7.4947e-02,\n",
       "         -1.5399e-01,  8.7530e-02,  2.8003e-02, -9.1623e-02,  1.6640e-02,\n",
       "         -2.3106e-02,  1.5098e-01, -1.7133e-02, -1.1216e-02,  9.4271e-02,\n",
       "         -6.7805e-02, -1.9409e-01, -2.2280e-01, -5.0848e-02,  7.6761e-02,\n",
       "          6.5501e-02, -4.5031e-02, -2.1115e-02,  3.7390e-02, -1.0103e-01,\n",
       "          6.8246e-02,  1.3508e-02, -1.6477e-01, -3.8458e-04,  3.2019e-03,\n",
       "          3.1034e-02,  6.9219e-02,  1.7332e-02, -6.0997e-02, -1.0081e-01,\n",
       "         -1.9389e-01,  3.2018e-02,  1.7577e-01,  5.0264e-02,  5.5658e-02,\n",
       "          2.0592e-01,  7.2404e-02,  5.2474e-02,  2.8370e-02, -5.2396e-03,\n",
       "         -9.5275e-03,  1.9911e-01, -7.7049e-03,  2.0102e-02, -1.5649e-01,\n",
       "         -5.7089e-02,  1.7805e-01, -1.3595e-01, -1.1524e-01,  8.9833e-02,\n",
       "          3.5364e-02, -7.3675e-02, -1.1083e-01, -1.8413e-01,  1.0882e-01,\n",
       "          9.3979e-02,  3.2813e-02, -7.1695e-05,  3.8555e-02,  1.2822e-02,\n",
       "          1.8763e-02, -1.3561e-02, -1.4448e-02, -1.3625e-01,  3.7390e-02,\n",
       "          4.6935e-02,  3.9548e-02,  8.4093e-03,  3.8228e-03, -9.3204e-02,\n",
       "         -1.1787e-01, -1.8435e-01, -1.9038e-02,  1.7300e-01,  2.8262e-02,\n",
       "         -5.8363e-02,  5.0004e-02,  5.4129e-02, -1.4330e-01,  1.3030e-01,\n",
       "         -9.3413e-02,  6.1946e-02, -1.5992e-01, -4.7372e-02,  1.8271e-02,\n",
       "          5.3608e-02, -1.1854e-02,  5.7583e-02, -2.1766e-02,  2.1296e-01,\n",
       "         -6.5985e-02,  7.7916e-02,  4.2725e-02,  5.4776e-02, -7.2360e-02,\n",
       "         -4.0527e-02,  4.6041e-02, -5.3366e-02, -8.0797e-02, -3.4414e-03,\n",
       "          1.1260e-01,  1.6080e-01,  1.0137e-01, -1.3764e-01, -2.5621e-02,\n",
       "          1.5336e-01, -1.3784e-01, -1.5705e-01, -9.9603e-02,  1.0961e-01,\n",
       "          6.8308e-02, -5.6495e-03,  4.6497e-02,  1.3540e-01, -1.1042e-01,\n",
       "         -7.0111e-02, -9.6111e-02,  8.2787e-03, -8.5187e-03, -4.3513e-02,\n",
       "         -1.1297e-01,  4.3624e-02, -6.1693e-02,  8.5424e-02,  4.1402e-02,\n",
       "          6.8118e-02,  9.3743e-02,  8.9228e-02,  1.1089e-01,  6.9143e-02,\n",
       "         -8.6081e-02, -1.0682e-01,  2.7848e-02, -1.8830e-01,  1.5486e-02,\n",
       "          8.9542e-02,  7.0112e-02,  6.0182e-02, -1.6216e-02, -9.6068e-02,\n",
       "          2.1079e-02, -1.6783e-02,  2.0036e-02, -3.7605e-02, -1.1112e-01,\n",
       "         -1.2643e-03, -4.4875e-02, -4.0219e-02, -1.6158e-01, -2.6065e-01,\n",
       "          3.6404e-02,  1.3255e-01,  4.7411e-02, -2.1936e-02, -1.4003e-01,\n",
       "          9.3334e-02,  1.1734e-01,  4.2576e-02,  3.4114e-02, -8.5348e-02,\n",
       "          9.7217e-03,  7.8143e-02,  2.5134e-02,  2.2137e-02, -6.6481e-02,\n",
       "          2.1609e-01,  3.2388e-03, -2.4682e-02, -3.3688e-03, -8.0001e-02,\n",
       "         -7.5215e-02, -1.6278e-02,  5.5735e-03,  6.6300e-03,  6.1396e-02,\n",
       "         -1.9484e-03,  1.8084e-01,  2.7027e-01,  4.9311e-02,  1.1671e-01,\n",
       "          1.8397e-03, -1.7359e-02,  8.1670e-02,  9.8629e-02, -3.2216e-02,\n",
       "          3.7110e-02, -3.1797e-02,  9.8946e-02,  1.0122e-01,  5.5936e-02,\n",
       "         -7.0260e-02, -1.0444e-01,  1.4873e-04,  5.7839e-02, -1.4537e-01,\n",
       "          2.6797e-01,  1.7392e-02, -7.2041e-03, -4.6829e-02, -2.8402e-02,\n",
       "         -1.8838e-01,  5.9867e-02, -5.3873e-02, -1.8152e-01, -3.7048e-02,\n",
       "         -5.7994e-02, -2.3919e-02,  2.1050e-02, -1.2682e-01, -6.4052e-03,\n",
       "          1.3441e-01, -1.4321e-01,  2.2577e-01,  1.2725e-02,  2.7243e-02,\n",
       "          9.5474e-02,  1.1919e-01, -3.2628e-02, -6.7936e-02,  1.0427e-01]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 224, 224)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'convnextv2_atto'\n",
    "model = ConvNeXtArcFace(model_name, embedding_size)\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "criterion = losses.ArcFaceLoss(num_classes=num_classes, embedding_size=embedding_size, margin=4).to(device)\n",
    "loss_optimizer = optim.Adam(criterion.parameters(), lr=loss_lr)\n",
    "\n",
    "start_epoch = 1\n",
    "\n",
    "def load_checkpoint(filepath, model, optimizer, loss_optimizer, criterion):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    criterion.load_state_dict(checkpoint['criterion_state_dict'])\n",
    "    loss_optimizer.load_state_dict(checkpoint['loss_optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch'] + 1\n",
    "    loss = checkpoint['loss']\n",
    "    return model, optimizer, loss_optimizer, criterion, epoch, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = None # \"epoch_5.pth\"\n",
    "if checkpoint:\n",
    "    model, optimizer, loss_optimizer, criterion, start_epoch, loss = load_checkpoint(\n",
    "        f\"checkpoints/{checkpoint}\", model, optimizer, loss_optimizer, criterion\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_embeddings(dataset, model):\n",
    "    tester = testers.BaseTester()\n",
    "    return tester.get_all_embeddings(dataset, model)\n",
    "\n",
    "accuracy_calculator = AccuracyCalculator(include=(\"precision_at_1\",), k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 1/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 1, Batch: 2/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 1, Batch: 3/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 1, Batch: 4/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 1, Batch: 5/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 1, Batch: 6/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 1, Batch: 7/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 1, Batch: 8/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 1, Batch: 9/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 1, Batch: 10/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 1, Batch: 11/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 1, Batch: 12/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 1, Batch: 13/13\n",
      "Embeddings: torch.Size([50, 320])\n",
      "\u001b[91mEpoch [1/500], Loss: 6.559036731719971\u001b[0m\n",
      "Epoch: 2, Batch: 1/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 2, Batch: 2/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 2, Batch: 3/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 2, Batch: 4/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 2, Batch: 5/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 2, Batch: 6/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 2, Batch: 7/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 2, Batch: 8/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 2, Batch: 9/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 2, Batch: 10/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 2, Batch: 11/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 2, Batch: 12/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 2, Batch: 13/13\n",
      "Embeddings: torch.Size([50, 320])\n",
      "\u001b[91mEpoch [2/500], Loss: 6.154073238372803\u001b[0m\n",
      "Epoch: 3, Batch: 1/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 3, Batch: 2/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 3, Batch: 3/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 3, Batch: 4/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 3, Batch: 5/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 3, Batch: 6/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 3, Batch: 7/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 3, Batch: 8/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 3, Batch: 9/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 3, Batch: 10/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 3, Batch: 11/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 3, Batch: 12/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 3, Batch: 13/13\n",
      "Embeddings: torch.Size([50, 320])\n",
      "\u001b[91mEpoch [3/500], Loss: 5.857439994812012\u001b[0m\n",
      "Epoch: 4, Batch: 1/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 4, Batch: 2/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 4, Batch: 3/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 4, Batch: 4/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 4, Batch: 5/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 4, Batch: 6/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 4, Batch: 7/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 4, Batch: 8/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 4, Batch: 9/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 4, Batch: 10/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 4, Batch: 11/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 4, Batch: 12/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 4, Batch: 13/13\n",
      "Embeddings: torch.Size([50, 320])\n",
      "\u001b[91mEpoch [4/500], Loss: 5.840883731842041\u001b[0m\n",
      "Epoch: 5, Batch: 1/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 5, Batch: 2/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 5, Batch: 3/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 5, Batch: 4/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 5, Batch: 5/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 5, Batch: 6/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 5, Batch: 7/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 5, Batch: 8/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 5, Batch: 9/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 5, Batch: 10/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 5, Batch: 11/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 5, Batch: 12/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 5, Batch: 13/13\n",
      "Embeddings: torch.Size([50, 320])\n",
      "\u001b[91mEpoch [5/500], Loss: 5.502309322357178\u001b[0m\n",
      "Epoch: 6, Batch: 1/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 6, Batch: 2/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 6, Batch: 3/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 6, Batch: 4/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 6, Batch: 5/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 6, Batch: 6/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 6, Batch: 7/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 6, Batch: 8/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 6, Batch: 9/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 6, Batch: 10/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 6, Batch: 11/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 6, Batch: 12/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 6, Batch: 13/13\n",
      "Embeddings: torch.Size([50, 320])\n",
      "\u001b[91mEpoch [6/500], Loss: 5.902864933013916\u001b[0m\n",
      "Epoch: 7, Batch: 1/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 7, Batch: 2/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 7, Batch: 3/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 7, Batch: 4/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 7, Batch: 5/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 7, Batch: 6/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 7, Batch: 7/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 7, Batch: 8/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 7, Batch: 9/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 7, Batch: 10/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 7, Batch: 11/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 7, Batch: 12/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 7, Batch: 13/13\n",
      "Embeddings: torch.Size([50, 320])\n",
      "\u001b[91mEpoch [7/500], Loss: 5.589833736419678\u001b[0m\n",
      "Epoch: 8, Batch: 1/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 8, Batch: 2/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 8, Batch: 3/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 8, Batch: 4/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 8, Batch: 5/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 8, Batch: 6/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 8, Batch: 7/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 8, Batch: 8/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 8, Batch: 9/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 8, Batch: 10/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 8, Batch: 11/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 8, Batch: 12/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 8, Batch: 13/13\n",
      "Embeddings: torch.Size([50, 320])\n",
      "\u001b[91mEpoch [8/500], Loss: 5.525914192199707\u001b[0m\n",
      "Epoch: 9, Batch: 1/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 9, Batch: 2/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 9, Batch: 3/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 9, Batch: 4/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 9, Batch: 5/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 9, Batch: 6/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 9, Batch: 7/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 9, Batch: 8/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 9, Batch: 9/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 9, Batch: 10/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 9, Batch: 11/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 9, Batch: 12/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 9, Batch: 13/13\n",
      "Embeddings: torch.Size([50, 320])\n",
      "\u001b[91mEpoch [9/500], Loss: 6.06682825088501\u001b[0m\n",
      "Epoch: 10, Batch: 1/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 10, Batch: 2/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 10, Batch: 3/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 10, Batch: 4/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 10, Batch: 5/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 10, Batch: 6/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 10, Batch: 7/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 10, Batch: 8/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 10, Batch: 9/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 10, Batch: 10/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 10, Batch: 11/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 10, Batch: 12/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 10, Batch: 13/13\n",
      "Embeddings: torch.Size([50, 320])\n",
      "\u001b[91mEpoch [10/500], Loss: 5.4297943115234375\u001b[0m\n",
      "Epoch: 11, Batch: 1/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 11, Batch: 2/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 11, Batch: 3/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 11, Batch: 4/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 11, Batch: 5/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 11, Batch: 6/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 11, Batch: 7/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 11, Batch: 8/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 11, Batch: 9/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 11, Batch: 10/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 11, Batch: 11/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 11, Batch: 12/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 11, Batch: 13/13\n",
      "Embeddings: torch.Size([50, 320])\n",
      "\u001b[91mEpoch [11/500], Loss: 5.566263675689697\u001b[0m\n",
      "Epoch: 12, Batch: 1/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 12, Batch: 2/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 12, Batch: 3/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 12, Batch: 4/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 12, Batch: 5/13\n",
      "Embeddings: torch.Size([64, 320])\n",
      "Epoch: 12, Batch: 6/13\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     12\u001b[0m loss_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 14\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# print(\"Embeddings:\", embeddings.shape)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(embeddings, targets)\n",
      "File \u001b[1;32mc:\\Users\\lefte\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lefte\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[58], line 8\u001b[0m, in \u001b[0;36mConvNeXtArcFace.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m----> 8\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[0;32m      9\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mavg_pool2d(x, \u001b[38;5;241m7\u001b[39m)\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbeddings:\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\lefte\\miniforge3\\lib\\site-packages\\timm\\models\\convnext.py:477\u001b[0m, in \u001b[0;36mConvNeXt.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    476\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstem(x)\n\u001b[1;32m--> 477\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    478\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_pre(x)\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\lefte\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lefte\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lefte\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\lefte\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lefte\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lefte\\miniforge3\\lib\\site-packages\\timm\\models\\convnext.py:233\u001b[0m, in \u001b[0;36mConvNeXtStage.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    231\u001b[0m     x \u001b[38;5;241m=\u001b[39m checkpoint_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, x)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 233\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\lefte\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lefte\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lefte\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\lefte\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lefte\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lefte\\miniforge3\\lib\\site-packages\\timm\\models\\convnext.py:155\u001b[0m, in \u001b[0;36mConvNeXtBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_conv_mlp:\n\u001b[0;32m    154\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m--> 155\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    157\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lefte\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lefte\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lefte\\miniforge3\\lib\\site-packages\\timm\\layers\\mlp.py:257\u001b[0m, in \u001b[0;36mGlobalResponseNormMlp.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    255\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x)\n\u001b[0;32m    256\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop1(x)\n\u001b[1;32m--> 257\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    258\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[0;32m    259\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop2(x)\n",
      "File \u001b[1;32mc:\\Users\\lefte\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lefte\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lefte\\miniforge3\\lib\\site-packages\\timm\\layers\\grn.py:39\u001b[0m, in \u001b[0;36mGlobalResponseNorm.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     37\u001b[0m x_g \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mnorm(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspatial_dim, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     38\u001b[0m x_n \u001b[38;5;241m=\u001b[39m x_g \u001b[38;5;241m/\u001b[39m (x_g\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_dim, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps)\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwb_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwb_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_n\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ckpt = [] # [1, 3, 5, 10, 15, 25, 40, 60, 80, 90, 110, 130, 150, 175]\n",
    "for epoch in range(start_epoch, epochs+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        print(f\"Epoch: {epoch}, Batch: {batch_idx + 1}/{len(train_loader)}\")\n",
    "        inputs = inputs.to(device)        \n",
    "        targets = targets.to(device)\n",
    "        inputs = inputs.float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_optimizer.zero_grad()\n",
    "\n",
    "        embeddings = model(inputs)\n",
    "        # print(\"Embeddings:\", embeddings.shape)\n",
    "        loss = criterion(embeddings, targets)\n",
    "        if log:    \n",
    "            writer.add_scalar('Loss/train', loss.item(), (epoch-1) * len(train_loader) + batch_idx + 1)  \n",
    "          \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "\n",
    "    # train_embeddings, train_labels = get_all_embeddings(train_dataset, model)\n",
    "    # val_embeddings, val_labels = get_all_embeddings(val_dataset, model)\n",
    "\n",
    "    # train_labels = train_labels.squeeze(1)\n",
    "    # val_labels = val_labels.squeeze(1)\n",
    "\n",
    "    # accuracies = accuracy_calculator.get_accuracy(\n",
    "    #         train_embeddings, train_labels, train_embeddings, train_labels, False\n",
    "    #     )\n",
    "    # training_accuracy = accuracies['precision_at_1']\n",
    "    # if log:\n",
    "    #     writer.add_scalar('Accuracy/Training', training_accuracy, epoch)\n",
    "    # print(f\"\\033[92mTrain Set Accuracy = {training_accuracy}\\033[0m\")\n",
    "\n",
    "    # accuracies = accuracy_calculator.get_accuracy(\n",
    "    #         val_embeddings, val_labels, train_embeddings, train_labels, False\n",
    "    #     )\n",
    "    # validation_accuracy = accuracies['precision_at_1']\n",
    "    # if log:\n",
    "    #     writer.add_scalar('Accuracy/Validation', validation_accuracy, epoch)\n",
    "    # print(f\"\\033[92mTest Set Accuracy = {validation_accuracy}\\033[0m\")\n",
    "\n",
    "\n",
    "    if (epoch) in ckpt:\n",
    "        torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss_optimizer_state_dict': loss_optimizer.state_dict(),\n",
    "                    'criterion_state_dict': criterion.state_dict(),\n",
    "                    'loss': running_loss,\n",
    "                    }, f\"checkpoints/epoch_{epoch}.pth\")\n",
    "        if log:\n",
    "            writer.flush()\n",
    "\n",
    "\n",
    "    print(f\"\\033[91mEpoch [{epoch}/{epochs}], Loss: {loss.item()}\\033[0m\")\n",
    "\n",
    "if log:\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'convnext_atto_arcface.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvNeXtArcFace(model_name, embedding_size)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('convnext_atto_arcface.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = RobustImageFolder('../faces/split/test', transform=transform)\n",
    "test_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_embeddings(dataset, model):\n",
    "    tester = testers.BaseTester()\n",
    "    return tester.get_all_embeddings(dataset, model)\n",
    "\n",
    "accuracy_calculator = AccuracyCalculator(include=(\"precision_at_1\",), k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings, train_labels = get_all_embeddings(train_dataset, model)\n",
    "test_embeddings, test_labels = get_all_embeddings(test_dataset, model)\n",
    "\n",
    "train_labels = train_labels.squeeze(1)\n",
    "test_labels = test_labels.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing accuracy...\")\n",
    "# accuracies = accuracy_calculator.get_accuracy(\n",
    "        # test_embeddings, test_labels, train_embeddings, train_labels, False\n",
    "    # )\n",
    "# print(\"Test set accuracy = {}\".format(accuracies[\"precision_at_1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: torch.Size([1, 320])\n",
      "Features: torch.Size([1, 320])\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "dummy_input = torch.randn(1, 3, 224, 224, requires_grad=True).to(device)\n",
    "dummy_output = model(dummy_input).to(device)\n",
    "torch.onnx.export(model, dummy_input, \"convnext_atto_arcface.onnx\", export_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onnx_model = onnx.load(\"convnext_atto_arcface.onnx\")\n",
    "# onnx.checker.check_model(onnx_model) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ort_session = onnxruntime.InferenceSession(\"convnext_atto_arcface.onnx\", providers=[\"CUDAExecutionProvider\"])\n",
    "\n",
    "# def to_numpy(tensor):\n",
    "#     return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(dummy_input)}\n",
    "# ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# # np.testing.assert_allclose(to_numpy(dummy_output), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
    "# # print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
